---
layout: "post"
title: "Catastrophic Forgetting"
date: "2018-12-01 11:11"
comments: true
desc: A tweet-storm on Catastrophic Forgetting
author: Sujith Jay Nair
tags: deep-learning tweetstorm
permalink: /catastrophic-forgetting
---
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">1/ Catastrophic Forgetting is a long-recognised problem in neural networks; and is of great interest in cognitive sciences. In plain words, it is the destructive interference effect of learning a new skill on pre-existing skills. <a href="https://twitter.com/hashtag/deeplearning?src=hash&amp;ref_src=twsrc%5Etfw">#deeplearning</a></p>&mdash; Sujith Jay Nair (@suj1th) <a href="https://twitter.com/suj1th/status/1068759316000210944?ref_src=twsrc%5Etfw">December 1, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">2/ Research in Deep Learning has had a particular focus on this problem in recent time, particularly in the realm of reinforcement learning. e.g. [Rolnick, Ahuja, Schwarz et al. 2018], [Shin, Lee, Kim et al. 2017] among others. <a href="https://twitter.com/hashtag/deeplearning?src=hash&amp;ref_src=twsrc%5Etfw">#deeplearning</a></p>&mdash; Sujith Jay Nair (@suj1th) <a href="https://twitter.com/suj1th/status/1068759317002604544?ref_src=twsrc%5Etfw">December 1, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">3/ A common thread in these research is the replay of past data to reinforce acquired skills from the past. Rolnick et al. (<a href="https://t.co/jluXqvN2Qr">https://t.co/jluXqvN2Qr</a>) choose a 50-50 split of replay vs. new task data. <a href="https://twitter.com/hashtag/deeplearning?src=hash&amp;ref_src=twsrc%5Etfw">#deeplearning</a></p>&mdash; Sujith Jay Nair (@suj1th) <a href="https://twitter.com/suj1th/status/1068759317942173696?ref_src=twsrc%5Etfw">December 1, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<!--break-->
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">4/ The use of replay using a limited buffer demonstrates a drastic reduction in catastrophic forgetting. The limited replay buffer result, in particular, is an exciting one; considering that deep learning models are such resource-hogs. <a href="https://twitter.com/hashtag/deeplearning?src=hash&amp;ref_src=twsrc%5Etfw">#deeplearning</a></p>&mdash; Sujith Jay Nair (@suj1th) <a href="https://twitter.com/suj1th/status/1068759319011704833?ref_src=twsrc%5Etfw">December 1, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">5/ This research, in cumulation with its predecessors, answers an important question about replay: a limited sample of past experiences produces a minimal difference in performance in comparison to an unlimited buffer. <a href="https://twitter.com/hashtag/deeplearning?src=hash&amp;ref_src=twsrc%5Etfw">#deeplearning</a></p>&mdash; Sujith Jay Nair (@suj1th) <a href="https://twitter.com/suj1th/status/1068759320098033664?ref_src=twsrc%5Etfw">December 1, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">6/ An important direction this experiment could take is via tweaks on the split of replay vs new tasks data, and uniform reservoir sampling on the replay buffer. Anybody who has tried to learn using spaced repetition knows that a uniform split at every point in the...</p>&mdash; Sujith Jay Nair (@suj1th) <a href="https://twitter.com/suj1th/status/1068759321062727681?ref_src=twsrc%5Etfw">December 1, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">7/ ...future does not guarantee optimality (or, so they say). A least-recently-used bias in sampling should help in achieving a spaced-repetition kind of behavior in replay. It would be interesting to see the effects of it on catastrophic forgetting. <a href="https://twitter.com/hashtag/deeplearning?src=hash&amp;ref_src=twsrc%5Etfw">#deeplearning</a></p>&mdash; Sujith Jay Nair (@suj1th) <a href="https://twitter.com/suj1th/status/1068759321951916033?ref_src=twsrc%5Etfw">December 1, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
