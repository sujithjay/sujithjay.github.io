---
layout: post
comments: true
title: Shuffle Hash Joins and Sort Merge Joins in Apache Spark
desc: An overview into Shuffle Hash Joins and Sort Merge Joins in Apache Spark SQL
author: Sujith Jay Nair
series: Apache Spark SQL
categories:
  - spark-sql
tags: apache-spark sql joins
permalink: /spark-sql/2018/06/28/Shuffle-Hash-Joins-and-Sort-Merge-Joins-in-Apache-Spark/
---

## Introduction
This post is the second in my series on Joins in Apache Spark SQL. The [first part](/spark-sql/2018/02/17/Broadcast-Hash-Joins-in-Apache-Spark/) explored Broadcast Hash Join; this post will focus on Shuffle Hash Joins & Sort Merge Joins.

<!--break-->
Although Broadcast Hash Join is the most performant join strategy, it is applicable to a small set of scenarios. Shuffle Hash Joins & Sort Merge Joins are the true work-horses of Spark SQL; a majority of the use-cases involving joins you will encounter in Spark SQL will have a physical plan using either of these strategies.

## [MCVE](https://stackoverflow.com/help/mcve)
Let us take an example to understand the join strategies better. This time we will be using the [Mondrian Foodmart](https://github.com/OSBI/foodmart-data) dataset to write our queries against. For those unaware of it, the foodmart dataset is a popular test dataset for OLAP scenarios. It originated as part of the test suite of the Pentaho Mondrian OLAP engine. You can check out its schema layout [here](https://github.com/julianhyde/foodmart-data-hsqldb/blob/master/foodmart-schema.png). We will be concerned with only a couple of tables from the dataset: _sales_fact_98_ & _customer_.

In Spark REPL, you can create the tables as shown below:
{% gist 1a33265eeb3598340722fca3e40fbba2 Initialise.md %}

The _explain_ output on the join-table describes the physical plan of the join operation:
{% gist 1a33265eeb3598340722fca3e40fbba2 ExplainOutput.md %}

The Spark SQL planner chooses to implement the join operation using _'SortMergeJoin'_. The [precedence order](https://github.com/apache/spark/blob/v2.2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala#L90-L120) for equi-join implementations (as in Spark 2.2.0) is as follows:
- _[Broadcast Hash Join](/spark-sql/2018/02/17/Broadcast-Hash-Joins-in-Apache-Spark/)_
- _Shuffle Hash Join_: if the average size of a single partition is small enough to build a hash table.
- _Sort Merge_: if the matching join keys are sortable.

## Pick One, Please
There is some confusion over the choice between Shuffle Hash Joins & Sort Merge Joins, particularly after Spark 2.3. Part of the reason is the introduction of a new configuration [spark.sql.join.preferSortMergeJoin](https://github.com/apache/spark/blob/v2.3.0/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala?utf8=%E2%9C%93#L157-L161), which is internal, and is set by default.

This means that Sort Merge is chosen every time over Shuffle Hash in Spark 2.3.0.

The preference of Sort Merge over Shuffle Hash in Spark is an ongoing discussion which has seen Shuffle Hash going in and out of Spark's join implementations multiple times. It was first removed from Spark in version [1.6.0](https://issues.apache.org/jira/browse/SPARK-11675). It made a comeback in [2.0.0](https://issues.apache.org/jira/browse/SPARK-13977). In 2.3.0, it has again been voted out in favour of Sort Merge.

A reason for the preference of Sort Merge is that it is considered a more robust implementation, as Shuffle Hash Join requires the hashed table to fit in memory, counter to Sort Merge Join which can spill to disk. But Shuffle Hash does have its benefits, particularly when the build side is much smaller than stream side. In that case, the building of a hash table on smaller side should be faster than sorting the bigger side. And given this clear benefit, I am sure Shuffle Hash will rise again from the ashes.

## Deep Dive
I would like to spend this section on Sort Merge Join alone, since its presence is invariant across Spark versions. The implementation of Sort Merge Join in Spark is similar to any other SQL engine, except that it happens over partitions because of the distributed nature of data. This means the best performance of this strategy is when the rows corresponding to same join-key are co-located. In every other case, it involves a shuffle operation to co-locate the data. We will have more to say on performance in the Caveats section.

I have below the code generated by Spark to perform the sort & merge operations. I give it to you without comment.

{% gist 1a33265eeb3598340722fca3e40fbba2 Sort.md %}
{% gist 1a33265eeb3598340722fca3e40fbba2 Merge.md %}

## Caveats

The performance of Sort Merge Join, as with every distributed join strategy, is optimal under certain conditions and sub-par under certain others.

#### _Best case scenarios_ :
In deteriorating order,
1. The Datasets have a known, shared partitioner; if the Datasets sharing the same partitioner are materialized by the same action, they will end up being co-located.
2. The Datasets are distributed evenly on the join columns.
3. The number of keys (combinations of join column values) is adequately large for the cluster and data-size at hand (since parallelism is proportional to the number of unique keys).

#### _Worst case scenarios_ :
1. Extremely uneven sharding of Datasets on the join columns.
2. A large Dataset is joined with another Dataset, such that a majority of the rows of larger Dataset are not relevant to the join condition. (In this case, these non-relevant rows of the large Dataset will still be shuffled across before being filtered out; hence, the performance hit.)


Upcoming posts in this series will explore Cartesian Product, Broadcast Nested Loop Joins and others. Tune in for them. Please leave a comment for suggestions, opinions, or just to say hello. Until next time!
